import { config } from "dotenv";
import { tool } from "@langchain/core/tools";
import * as z from "zod";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { SystemMessage, ToolMessage } from "@langchain/core/messages";

config();

const llm = new ChatGoogleGenerativeAI({
  model: "gemini-2.5-flash",
  apiKey: process.env.GEMINI_API_KEY,
});

const multiply = tool(
  async ({ a, b }) => {
    return a * b;
  },
  {
    name: "Multiply",
    description: "Multiply two numbers",
    schema: z.object({
      a: z.number().describe("first number"),
      b: z.number().describe("second number"),
    }),
  }
);

const add = tool(
  ({ a, b }) => {
    return a + b;
  },
  {
    name: "add",
    description: "Add two numbers together",
    schema: z.object({
      a: z.number().describe("first number"),
      b: z.number().describe("second number"),
    }),
  }
);

const divide = tool(
  ({ a, b }) => {
    return a / b;
  },
  {
    name: "divide",
    description: "Divide two numbers",
    schema: z.object({
      a: z.number().describe("first number"),
      b: z.number().describe("second number"),
    }),
  }
);

const availableTools = [add, multiply, divide];
const toolsByName = Object.fromEntries(
  availableTools.map((tool) => [tool.name, tool])
);

// => [['add', add], ['Multiply', multiply], ['divide', divide]]
// {
//   add: add,
//   Multiply: multiply,
//   divide: divide
// }

const llmWithTools = llm.bindTools(availableTools);

// What is state?

// state is an object that LangGraph automatically passes to all the functions
// It contains: state.messages which is an array of all conversation messages

// state = {
//   messages: [
//     { role: "user", content: "Add 3 and 4" }
//   ]
// }

// State is an object that is passed by LangGraph to every function in the agent workflow, but what that state contains - that we can decide. For example, here we did new StateGraph(MessagesAnnotation), so it tells state will have a messages attribute which is an array. This way we could have used any other configuration which tells state will contain attributes like fname, lname, email, etc.

// Key point: MessagesAnnotation is a predefined schema from LangGraph specifically for message-based conversations. For custom attributes, you'd define your own state schema!

async function llmCall(state) {
  // LLM decides whether to call a tool or not
  const result = await llmWithTools.invoke([
    {
      role: "system",
      content:
        "You are a helpful assistant tasked with performing arithmetic on a set of inputs.",
    }, // system instructions
    ...state.messages, // All previous messages (user messages, AI responses, tool results)
  ]);

  return {
    messages: [result],
    //The LLM responds with result =>  ans is 7 or request an tool, this gets added to msgs array. When we return { messages: [result] }, LangGraph appends this to the existing state.messages. It doesn't replace them!
  };
}

const toolNode = new ToolNode(availableTools);

// What is ToolNode?

// It's a ready-made node that LangGraph provides
// Its job: Execute the tools that the LLM asks for

// How does it work?
// When the LLM says "I want to use the add tool with a=3, b=4":

// ToolNode looks at the tool call
// Finds the `add` tool from availableTools
// Executes it: add(3, 4)
// Gets the result: 7
// Creates a message with this result
// Adds this message to state.messages

// Ex --
// {
//   role: "tool",
//   content: "7",  // The result of add(3, 4)
//   name: "add"
// }

// Conditional edge function to route to the tool node or end
function shouldContinue(state) {
  const messages = state.messages;
  const lastMessage = messages.at(-1);

  // When the LLM wants to use a tool, it adds tool_calls to its response:
  if (lastMessage?.tool_calls?.length) {
    return "toolNode";
  }
  // Otherwise, we stop (reply to the user)
  return "__end__";
}


// shouldContinue doesn't execute the toolNode - it just tells the graph WHERE to go next!

// Here's how it works:

// shouldContinue returns a string: "toolNode" or "__end__"
// LangGraph reads that string and says "okay, go to the node with that name"
// LangGraph then executes the actual toolNode
// toolNode runs, executes the tool, creates a message, and adds it to state
// Build workflow


const agentBuilder = new StateGraph(MessagesAnnotation)
// Creates a new graph
// MessagesAnnotation means: "The state will have a messages property that's an array

   // Add nodes 
  .addNode("llmCall", llmCall) // Add a node called "llmCall" that runs the llmCall function
  .addNode("toolNode", toolNode) // Add a node called "toolNode" that runs the toolNode

  // Add edges to connect nodes
  .addEdge("__start__", "llmCall") // When we start, always go to "llmCall" first
  .addConditionalEdges("llmCall", shouldContinue, ["toolNode", "__end__"]) 
      //   From: "llmCall" node
      // Use: shouldContinue function to decide where to go --> toolNode or __end__
      // 3rd argument = List of possible destination nodes shouldContinue can return,It tells LangGraph: "These are the ONLY valid places shouldContinue can route to. if it returns something else (e.g., "randomNode") â†’ ERROR! âŒ (not in the list)
  .addEdge("toolNode", "llmCall") // After executing tools, go back to llmCall
  .compile(); // Build the final graph



//   If returns "toolNode":
// Graph goes to toolNode â†’ executes tools â†’ then follows the edge toolNode â†’ llmCall (defined in .addEdge("toolNode", "llmCall"))

// If returns "__end__":
// Graph stops, conversation ends, final state.messages is returned to user as output

// Invoke
const messages = [
  {
    role: "user",
    content: "Add 3 and 4.",
  },
];
const result = await agentBuilder.invoke({ messages });
// We invoke the graph with this initial state,so, This is the initial state of state.messages when you invoke the graph.
console.log(result.messages);


// ## **The Complete Flow (Visual)**
// ```
// START
//   â†“
// llmCall (LLM thinks)
//   â†“
// shouldContinue() checks: "Did LLM request tools?"
//   â†“                    â†“
//   YES                  NO
//   â†“                    â†“
// toolNode            __END__
// (execute tools)
//   â†“
//   â†“ (loop back)
// llmCall (LLM sees tool results)
//   â†“
// shouldContinue() checks again
//   â†“
//   NO (this time)
//   â†“
// __END__


// ### **Execution Trace:**

// **ðŸ”µ Round 1:**
// 1. **START â†’ llmCall**
//    - State: `messages = [{ role: "user", content: "Add 3 and 4." }]`
//    - LLM receives: system message + user message
//    - LLM thinks: "I need to use add tool"
//    - LLM returns: `{ role: "assistant", tool_calls: [{ name: "add", args: { a: 3, b: 4 } }] }`
//    - This gets added to state.messages

// 2. **llmCall â†’ shouldContinue**
//    - Checks last message
//    - Sees `tool_calls` exists
//    - Returns `"toolNode"`

// 3. **Goes to toolNode**
//    - Executes `add(3, 4)` â†’ gets `7`
//    - Adds: `{ role: "tool", content: "7" }`
//    - Now state.messages has: user message, assistant tool call, tool result

// 4. **toolNode â†’ llmCall** (loop back)

// **ðŸ”µ Round 2:**
// 5. **llmCall again**
//    - State now has: user message, AI tool call, tool result (7)
//    - LLM sees the tool returned 7
//    - LLM thinks: "Great! I have the answer"
//    - LLM returns: `{ role: "assistant", content: "The answer is 7." }`
//    - No `tool_calls` this time!

// 6. **llmCall â†’ shouldContinue**
//    - Checks last message
//    - No `tool_calls` found
//    - Returns `"__end__"`

// 7. **Goes to END**

// **Step 4:** Final result:
// ```javascript
// result.messages = [
//   { role: "user", content: "Add 3 and 4." },
//   { role: "assistant", tool_calls: [...] },
//   { role: "tool", content: "7" },
//   { role: "assistant", content: "The answer is 7." }
// ]
// ```


// role: "user" â†’ Messages from the human user (queries, requests)
// role: "assistant" â†’ Messages from the AI/LLM (responses, tool calls)
// role: "system" â†’ Instructions/context for the AI (tells it how to behave)
// role: "tool" â†’ Results returned by tools after execution (e.g., "7" from add function)

// So: role: "tool" = convention for tool results, not specific to ToolNode. So whether we use the prebuilt ToolNode or we manually do tool execution, for the results of tools we always put role: "tool"

// role: "assistant"

// Comes from LLM (the llmCall node)
// This is the AI's response
// Can contain text OR tool calls

role: "tool"

// Comes from ToolNode (after executing the tool)
// Always contains the tool's return value

// assistant = What the AI SAYS or REQUESTS
// tool = What the tool RETURNS



// result.messages is the EXACT same state.messages that LangGraph maintained throughout the entire workflow!When you do console.log(result.messages), you're seeing the complete conversation history that accumulated as the graph executed.

// The messages array contains:
// User message (your initial input)
// Assistant message with tool call to add(3, 4)
// Tool message with result 7
// Assistant message with tool call to multiply(7, 10)
// Tool message with result 70
// Assistant message with tool call to divide(70, 2)
// Tool message with result 35
// Assistant message with tool call to add(35, 5)
// Tool message with result 40
// Assistant message with final answer "The answer is 40"


// but then why it looks so complex?

// console.log(result.messages);

// This prints the raw message objects with:
// All internal LangChain data structures

// LangChain message objects contain:

// id: Unique identifier
// content: The actual message
// additional_kwargs: Extra info from the LLM
// response_metadata: Token usage, model info
// tool_calls: Tool requests
// usage_metadata: Billing/usage stats

// You only need content and tool_calls for reading!

// so we can do this

// result.messages.forEach((msg, i) => {
//   const type = msg._getType();
//   const content = msg.content || JSON.stringify(msg.tool_calls);
//   console.log(`${i + 1}. [${type}]: ${content}`);
// });

// OR

// const finalAnswer = result.messages.at(-1);
// console.log(finalAnswer.content);
// Output: "The final result is 40."

//imp Qs

// ## **1. Is `shouldContinue` a node?**

// **NO! `shouldContinue` is NOT a node.**

// ### **What is it then?**

// `shouldContinue` is a **routing function** (also called a **conditional edge function** or **decision function**).

// ### **Difference:**

// - **Nodes** = Do work (execute logic, call LLM, run tools, modify state)
//   - `llmCall` â†’ calls the LLM
//   - `toolNode` â†’ executes tools

// - **Routing functions** = Make decisions (just return a string to say where to go next, don't modify state)
//   - `shouldContinue` â†’ returns `"toolNode"` or `"__end__"`

// ### **Why not add it as a node?**

// Because it doesn't DO anything - it just DECIDES where to go!

// **Think of it like:**
// - **Nodes** = Workers doing tasks ðŸ‘·
// - **Routing functions** = Traffic signs pointing directions ðŸš¦

// ---

// ## **2. What does `.compile()` do?**

// **It builds the final workflow.**

// ### **What compile does:**

// ```javascript
// .compile();  
// ```

// 1. **Takes all the nodes and edges** you defined
// 2. **Validates** the graph structure (checks for errors)
// 3. **Builds the executable workflow**
// 4. **Returns a runnable graph** that you can invoke

// ### **Before compile:**
// ```
// agentBuilder = [just a blueprint/recipe, can't run yet]
// ```

// ### **After compile:**
// ```
// agentBuilder = [executable workflow ready to run!]
// ```
